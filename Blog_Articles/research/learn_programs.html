<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8" />
  <title>Analysis and Math</title>
  <meta content="width=device-width, initial-scale=1.0" name="viewport" />
  <meta content="Free Website Template" name="keywords" />
  <meta content="Free Website Template" name="description" />

  <!-- Favicon -->
  <link href="img/favicon.ico" rel="icon" />

  <!-- Google Font -->
  <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400;500;600;700&display=swap"
    rel="stylesheet" />

  <!-- CSS Libraries -->
  <link href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css" rel="stylesheet" />
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.10.0/css/all.min.css" rel="stylesheet" />
  <link href="lib/animate/animate.min.css" rel="stylesheet" />
  <link href="lib/owlcarousel/assets/owl.carousel.min.css" rel="stylesheet" />
  <link href="lib/lightbox/css/lightbox.min.css" rel="stylesheet" />

  <!-- Template Stylesheet -->
  <link href="/css/style.css" rel="stylesheet" />
</head>

<body>
  <!-- Nav Bar Start -->
  <nav class="navbar navbar-expand-lg navbar-light position-fixed">
    <div class="container  justify-content-between">
        <a class="nav-item nav-link navbar-brand" href="/index.html#home">KOFFI IDONGESIT</a>
        <button class="navbar-toggler bg-white" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>
        <div class="collapse navbar-collapse flex-grow-0" id="navbarSupportedContent">
            <ul class="navbar-nav me-auto">
                <a class="nav-item nav-link" href="/index.html#home">Home</a>
                <a class="nav-item nav-link" href="/index.html#about" >About</a>
                <a href="/index.html#experience" class="nav-item nav-link">WHY ME</a>
                <a href="/index.html#blog" class="nav-item nav-link">Articles</a>
                <a href="/index.html#testimonial" class="nav-item nav-link">Reviews</a>
            </ul>
        </div>
    </div>
</nav>
  <!-- Nav Bar End -->

  <!-- Blog Start -->
  <div class="blog sub-blog" id="blog">
    <div class="container">
      <div class="section-header text-center wow zoomIn" data-wow-delay="0.1s">
        <!-- <p class="para">From Blog</p> -->
        <!-- <br /> -->
        <!-- <a class="btn" href="/sub_Blog/research.html"><i class="fa fa-angle-left"></i>Back</a> -->
      </div>
      <div class="p-3 write-ups wow zoomIn" data-wow-delay="0.1s">
          <h4>
            How to Learn Programs with Artificial Recurrent Neural Networks?            </h4>
          <h5 class="text-center">Abstract</h5>
          <p>
            Learning programs is a scheme for finding the optimal policy to control a system, based on a scalar signal representing a reward or a punishment. If the observation of the system by the controller is sufficiently rich to represent the internal state of the system, the controller can achieve the optimal policy simply by learning reactive behavior. However, if the state of the controlled system cannot be assessed completely using current sensory observations, the controller must learn a dynamic behavior to achieve the optimal policy. In this paper, we propose a dynamic controller scheme which utilizes memory to uncover hidden states by using information about past system outputs, and makes control decisions using memory. This scheme integrates Q-learning, as proposed by (Watkins, and recurrent neural networks of several types. It performs favorably in simulations which involve a task with hidden states. 
          </p>
          
            <h5 class="text-center">
                Introduction
            </h5>
            <p>
                Learning programs is an attractive method for autonomous learning controllers because it does not require a teacher to show the correct policy. It uses only a signal, i.e., a scalar performance indicator. It works in the following way. The controller observes the output of the system, and decides on an action. The system changes its state according to the action taken, and yields a new output, and a scalar signal to the controller. The aim is to find an optimal policy, i.e., a mapping of the output of the system to the action which maximizes a value function, usually the cumulative discounted sum of the signals. Several learning techniques have been proposed to realize learning program, e.g., TD learning as proposed by Sutton, 1 and Q-learning as proposed by (Watkins, et al, 1992).  ( Barto et al., 1993).  Have suggested a number of other possibilities. Studies on learning program have focused mainly on cases where the state of the controlled system is fully available to the controller as the output of the system, and have tried to obtain the optimal mapping of system outputs to control actions. However, in realistic situations, it is usually impossible to access the entire state variables of the system. Some of the states may be hidden from the controller because of sensory limitations. In other words, multiple system states may be perceived as the same by the controller. (Whitehead et al., 1991). Have called this perceptual aliasing. While perceptual aliasing may be advantageous when the aliased states are irrelevant to the task at hand, states requiring different actions might also be aliased. This impairs the performance of the controller, and is clearly harmful. Various methods have been proposed for learning program  in the presence of perceptual aliasing. A simple method is the fixed duration memory method, which stores the sensory information of the last n episodes. This method converts a temporal decision task into a spatial one, and therefore the learning techniques for reactive controllers " become available. However, this approach suffers from an exponential increase in the number of states because of the increased number of inputs, and the uncertainty in setting the length of the delay. Whitehead and Ballard 6 have proposed a reconfigurable sensorimotor system to utilize the available sensors more efficiently. Their "lion" algorithm also discovers and avoids passing through perceptually aliased states. (Tan, 1991). Tan 7 has proposed cost-sensitive learning programs in order to optimize the ways of using the available sensors. (Chrisman, 1992). ~ has proposed a method which prepares an internal model of the state space of the controlled system, and splits the aliased states in that model based on the statistics gathered. (McCallum, 1995). McCallum 9 has also used statistical tests to discriminate the aliased states, and uses variable length short-term memory to choose the correct task when it encounters one. (Littman, 1994) Littman t~ proposed memory less policies that only use instantaneous sensory observation. Our approach" is to use memory to retain important information about events in the recent past. A variation of Q-learning is proposed that works with a recurrent neural network. Recurrent neural networks (RNN) are neural networks that have feedback connections. Architectures and learning algorithms of RNN have been intensely investigated in the context of supervised learning schemes. In this paper, two types of RNN architecture, namely the Elman neural network (Elman JL 1990). 12 and the information loss minimization method proposed by (Noda, 1994). T3 are compared through computer simulation. Q-learning for recurrent neural networks In this paper, we propose a dynamic controller which reconstructs missing state information based on the past outputs of the controlled system. The controlled system is assumed to be finite-state, discrete-time, and is characterized by a Markovian process where the state transition probability is given by Prob [X(,+I)= x, lX,= xi,At=ak], t= 1,2 .... (1) where xi, xj E J' are the states, X, denotes the state of the system at time t, and A, = a k E c~/is the action performed at time t. The system returns the programs signal r(t.l ) (Xt, At) to the controller for action At from state X,. The observable output of the system Y = CX is calculated from the state vector X. We assume that at some points in time, X, cannot be determined by considering only Y,, but the system is observable, i.e., the state X, can be estimated from the past output {Y(,_~), Y(,-2) . . . . }. The aim of the controller is to find a policy that maximizes the cumulative discounted programs (or the utility) given by V t = Z~[kr(t+k) (2) k-0 where 0 < y -< 1 is a constant called the discount factor.
            </p>
           

        



      </div>
      <div class="align-content-center mt-2 text-center wow zoomIn" data-wow-delay="0.1s">
        <a class="btn" href="/sub_Blog/research.html"><i class="fa fa-angle-left"></i>Back</a>
      </div>
    </div>
  </div>
  <!-- Blog End -->

  <!-- Footer Start -->
  <div class="footer mt-0 wow fadeIn" data-wow-delay="0.3s">
    <div class="container-fluid">
      <div class="container">
        <div class="footer-info">
          <h2>Idongesit Koffi</h2>
          <div class="footer-menu">
            <p>+234-706-455-1848</p>
            <p>koffijnr@gmail.com</p>
          </div>
        </div>
      </div>
      <div class="container copyright">
        <p>
          &copy; <a href="#">Your Site Name</a>, All Right Reserved | Designed
          By <a href="https://ezekielelom.com">Ellovick</a>
        </p>
        <a href="#" class="btn back-to-top"><i class="fa fa-chevron-up"></i></a>
      </div>
    </div>
  </div>
  <!-- Footer End -->

  <!-- Back to top button -->
  <a href="#" class="btn back-to-top"><i class="fa fa-chevron-up"></i></a>

  <!-- Pre Loader -->
  <!-- <div id="loader" class="show">
         <div class="loader"></div>
     </div> -->

  <!-- JavaScript Libraries -->
  <script src="https://code.jquery.com/jquery-3.4.1.min.js"></script>
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.bundle.min.js"></script>

  <!-- <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-1BmE4kWBq78iYhFldvKuhfTAU6auU8tT94WrHftjDbrCEXSU1oBoqyl2QvZ6jIW3" crossorigin="anonymous"> -->

  <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.10.2/dist/umd/popper.min.js"
    integrity="sha384-7+zCNj/IqJ95wo16oMtfsKbZ9ccEh31eOz1HGyDuCQ6wgnyJNSYdrPa03rtR1zdB"
    crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.min.js"
    integrity="sha384-QJHtvGhmr9XOIpI6YVutG+2QOK9T+ZnN4kzFN1RtK3zEFEIsxhlmWl5/YESvpZ13"
    crossorigin="anonymous"></script>
  <script src="lib/easing/easing.min.js"></script>
  <script src="lib/wow/wow.min.js"></script>
  <script src="lib/waypoints/waypoints.min.js"></script>
  <script src="lib/typed/typed.min.js"></script>
  <script src="lib/owlcarousel/owl.carousel.min.js"></script>
  <script src="lib/isotope/isotope.pkgd.min.js"></script>
  <script src="lib/lightbox/js/lightbox.min.js"></script>

  <!-- Contact Javascript File -->
  <script src="mail/jqBootstrapValidation.min.js"></script>
  <script src="mail/contact.js"></script>

  <!-- Template Javascript -->
  <script src="js/main.js"></script>
</body>

</html>